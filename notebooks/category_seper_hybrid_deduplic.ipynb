{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b53be83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (4.66.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.46.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: sympy in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\madha\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-5.1.2\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad15ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madha\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\madha\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\madha\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\madha\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ Utility functions\n",
    "# -------------------------------\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for hashing and comparison.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)            # collapse whitespace\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)         # remove punctuation\n",
    "    return text.strip()\n",
    "\n",
    "def compute_hash(text: str) -> str:\n",
    "    \"\"\"Generate a unique hash ID for dedup tracking.\"\"\"\n",
    "    return hashlib.md5(normalize_text(text).encode()).hexdigest()\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ Load and group by category\n",
    "# -------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa16b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d7b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 categories:\n",
      " - Civil Law: 87 records\n",
      " - Family Law: 307 records\n",
      " - Criminal Law: 134 records\n",
      " - Labour: 32 records\n",
      " - Property Law: 224 records\n",
      " - Business Law: 17 records\n",
      " - Consumer Law: 2 records\n",
      " - Constitutional Law: 5 records\n",
      " - Taxation: 4 records\n",
      "\n",
      "üîπ Processing category: Civil Law\n",
      "   ‚ûú After exact dedup: 87 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.69s/it]\n",
      "Semantic dedup (Civil Law): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:00<00:00, 196.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûú After semantic dedup: 87 items\n",
      "   ‚úÖ Saved cleaned file: deduplicated_data\\Civil_Law_clean.json\n",
      "\n",
      "üîπ Processing category: Family Law\n",
      "   ‚ûú After exact dedup: 307 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:25<00:00,  2.60s/it]\n",
      "Semantic dedup (Family Law): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:05<00:00, 52.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûú After semantic dedup: 307 items\n",
      "   ‚úÖ Saved cleaned file: deduplicated_data\\Family_Law_clean.json\n",
      "\n",
      "üîπ Processing category: Criminal Law\n",
      "   ‚ûú After exact dedup: 134 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:12<00:00,  2.42s/it]\n",
      "Semantic dedup (Criminal Law): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 134/134 [00:01<00:00, 121.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûú After semantic dedup: 134 items\n",
      "   ‚úÖ Saved cleaned file: deduplicated_data\\Criminal_Law_clean.json\n",
      "\n",
      "üîπ Processing category: Labour\n",
      "   ‚ûú After exact dedup: 32 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.26s/it]\n",
      "Semantic dedup (Labour): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 484.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûú After semantic dedup: 32 items\n",
      "   ‚úÖ Saved cleaned file: deduplicated_data\\Labour_clean.json\n",
      "\n",
      "üîπ Processing category: Property Law\n",
      "   ‚ûú After exact dedup: 224 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:19<00:00,  2.78s/it]\n",
      "Semantic dedup (Property Law): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224/224 [00:03<00:00, 71.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûú After semantic dedup: 224 items\n",
      "   ‚úÖ Saved cleaned file: deduplicated_data\\Property_Law_clean.json\n",
      "\n",
      "üîπ Processing category: Business Law\n",
      "   ‚ûú After exact dedup: 17 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.60s/it]\n",
      "Semantic dedup (Business Law): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [00:00<00:00, 3397.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûú After semantic dedup: 17 items\n",
      "   ‚úÖ Saved cleaned file: deduplicated_data\\Business_Law_clean.json\n",
      "\n",
      "üîπ Processing category: Consumer Law\n",
      "   ‚ûú After exact dedup: 2 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.96it/s]\n",
      "Semantic dedup (Consumer Law): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûú After semantic dedup: 2 items\n",
      "   ‚úÖ Saved cleaned file: deduplicated_data\\Consumer_Law_clean.json\n",
      "\n",
      "üîπ Processing category: Constitutional Law\n",
      "   ‚ûú After exact dedup: 5 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.74it/s]\n",
      "Semantic dedup (Constitutional Law): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 3042.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûú After semantic dedup: 5 items\n",
      "   ‚úÖ Saved cleaned file: deduplicated_data\\Constitutional_Law_clean.json\n",
      "\n",
      "üîπ Processing category: Taxation\n",
      "   ‚ûú After exact dedup: 4 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.03it/s]\n",
      "Semantic dedup (Taxation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 3123.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûú After semantic dedup: 4 items\n",
      "   ‚úÖ Saved cleaned file: deduplicated_data\\Taxation_clean.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_file = \"C:/Users/madha/vsCode/Github/legal-assist-rag/data/test.json\"  # your JSON file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Group all items by their category\n",
    "category_data = defaultdict(list)\n",
    "for item in data:\n",
    "    category = item.get(\"query-category\", \"Unknown\").strip()\n",
    "    category_data[category].append(item)\n",
    "\n",
    "print(f\"Found {len(category_data)} categories:\")\n",
    "for cat, items in category_data.items():\n",
    "    print(f\" - {cat}: {len(items)} records\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ Deduplication setup\n",
    "# -------------------------------\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "output_dir = \"deduplicated_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ Process category-wise\n",
    "# -------------------------------\n",
    "\n",
    "for category, items in category_data.items():\n",
    "    print(f\"\\nüîπ Processing category: {category}\")\n",
    "\n",
    "    # Step 1: remove exact duplicates\n",
    "    seen_hashes = set()\n",
    "    unique_items = []\n",
    "    for item in items:\n",
    "        qtext = item.get(\"query-text\", \"\")\n",
    "        qhash = compute_hash(qtext)\n",
    "        if qhash not in seen_hashes:\n",
    "            unique_items.append(item)\n",
    "            seen_hashes.add(qhash)\n",
    "\n",
    "    print(f\"   ‚ûú After exact dedup: {len(unique_items)} items\")\n",
    "\n",
    "    # Step 2: semantic deduplication\n",
    "    query_texts = [i[\"query-text\"] for i in unique_items]\n",
    "    embeddings = model.encode(query_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    to_remove = set()\n",
    "    threshold = 0.85  # tune this based on test results\n",
    "\n",
    "    # Compare each query with the rest\n",
    "    for i in tqdm(range(len(unique_items)), desc=f\"Semantic dedup ({category})\"):\n",
    "        if i in to_remove:\n",
    "            continue\n",
    "        for j in range(i + 1, len(unique_items)):\n",
    "            if j in to_remove:\n",
    "                continue\n",
    "            sim = util.cos_sim(embeddings[i], embeddings[j]).item()\n",
    "            if sim > threshold:\n",
    "                # Mark j as duplicate (keep the first one)\n",
    "                to_remove.add(j)\n",
    "\n",
    "    final_items = [item for idx, item in enumerate(unique_items) if idx not in to_remove]\n",
    "\n",
    "    print(f\"   ‚ûú After semantic dedup: {len(final_items)} items\")\n",
    "\n",
    "    # Step 3: save category-wise cleaned data\n",
    "    output_path = os.path.join(output_dir, f\"{category.replace(' ', '_')}_clean.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_items, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"   ‚úÖ Saved cleaned file: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc81891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f96cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a9b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7578f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8f70e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
